{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import statsmodels.api as sm\n",
    "from forecasting_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_processed = pd.read_csv('data/processed_FRED_Data.csv')\n",
    "df_processed.head()\n",
    "variables = list(df_processed.select_dtypes(include=[np.number]).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /Users/sebastianquintero/Library/CloudStorage/OneDrive-QuinteroOrthodontics/MIT/MEng/rag-thesis/lag-llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /Users/sebastianquintero/Library/CloudStorage/OneDrive-QuinteroOrthodontics/MIT/MEng/rag-thesis/lag-llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download time-series-foundation-models/Lag-Llama lag-llama.ckpt --local-dir /Users/sebastianquintero/Library/CloudStorage/OneDrive-QuinteroOrthodontics/MIT/MEng/rag-thesis/lag-llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "import torch\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.dataset.repository.datasets import get_dataset\n",
    "\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "import pandas as pd\n",
    "\n",
    "from lag_llama.gluon.estimator import LagLlamaEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from types import ModuleType\n",
    "\n",
    "# Create dummy module hierarchy\n",
    "def create_dummy_module(module_path):\n",
    "    \"\"\"\n",
    "    Create a dummy module hierarchy for the given path.\n",
    "    Returns the leaf module.\n",
    "    \"\"\"\n",
    "    parts = module_path.split('.')\n",
    "    current = ''\n",
    "    parent = None\n",
    "\n",
    "    for part in parts:\n",
    "        current = current + '.' + part if current else part\n",
    "        if current not in sys.modules:\n",
    "            module = ModuleType(current)\n",
    "            sys.modules[current] = module\n",
    "            if parent:\n",
    "                setattr(sys.modules[parent], part, module)\n",
    "        parent = current\n",
    "\n",
    "    return sys.modules[module_path]\n",
    "\n",
    "# Create the dummy gluonts module hierarchy\n",
    "gluonts_module = create_dummy_module('gluonts.torch.modules.loss')\n",
    "\n",
    "# Create dummy classes for the specific loss functions\n",
    "class DistributionLoss:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return 0.0\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return lambda *args, **kwargs: None\n",
    "\n",
    "class NegativeLogLikelihood:\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return 0.0\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        return lambda *args, **kwargs: None\n",
    "\n",
    "# Add the specific classes to the module\n",
    "gluonts_module.DistributionLoss = DistributionLoss\n",
    "gluonts_module.NegativeLogLikelihood = NegativeLogLikelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lag_llama_predictions(dataset, prediction_length, device, context_length=32, use_rope_scaling=False, num_samples=100):\n",
    "    ckpt = torch.load(\"lag-llama.ckpt\", map_location=device, weights_only=False) # Uses GPU since in this Colab we use a GPU.\n",
    "    estimator_args = ckpt[\"hyper_parameters\"][\"model_kwargs\"]\n",
    "\n",
    "    rope_scaling_arguments = {\n",
    "        \"type\": \"linear\",\n",
    "        \"factor\": max(1.0, (context_length + prediction_length) / estimator_args[\"context_length\"]),\n",
    "    }\n",
    "\n",
    "    estimator = LagLlamaEstimator(\n",
    "        ckpt_path=\"lag-llama.ckpt\",\n",
    "        prediction_length=prediction_length,\n",
    "        context_length=context_length, # Lag-Llama was trained with a context length of 32, but can work with any context length\n",
    "\n",
    "        # estimator args\n",
    "        input_size=estimator_args[\"input_size\"],\n",
    "        n_layer=estimator_args[\"n_layer\"],\n",
    "        n_embd_per_head=estimator_args[\"n_embd_per_head\"],\n",
    "        n_head=estimator_args[\"n_head\"],\n",
    "        scaling=estimator_args[\"scaling\"],\n",
    "        time_feat=estimator_args[\"time_feat\"],\n",
    "        rope_scaling=rope_scaling_arguments if use_rope_scaling else None,\n",
    "\n",
    "        batch_size=1,\n",
    "        num_parallel_samples=100,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    lightning_module = estimator.create_lightning_module()\n",
    "    transformation = estimator.create_transformation()\n",
    "    predictor = estimator.create_predictor(transformation, lightning_module)\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        dataset=dataset,\n",
    "        predictor=predictor,\n",
    "        num_samples=num_samples\n",
    "    )\n",
    "    forecasts = list(forecast_it)\n",
    "    tss = list(ts_it)\n",
    "\n",
    "    return forecasts, tss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of numeric columns (these will be our variables to forecast)\n",
    "numeric_columns = df_processed.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# Melt the dataframe to convert from wide to long format\n",
    "df_long = pd.melt(\n",
    "    df_processed,\n",
    "    id_vars=['observation_date'],\n",
    "    value_vars=numeric_columns,\n",
    "    var_name='item_id',\n",
    "    value_name='target'\n",
    ")\n",
    "\n",
    "# Sort by item_id and date\n",
    "df_long = df_long.sort_values(['item_id', 'observation_date'])\n",
    "\n",
    "# Set the date as index without a name\n",
    "df_long = df_long.set_index('observation_date', drop=True)\n",
    "df_long.index.name = None\n",
    "\n",
    "# Display the first few rows to verify the transformation\n",
    "print(\"Shape of transformed data:\", df_long.shape)\n",
    "print(\"\\nFirst few rows of transformed data:\")\n",
    "print(df_long.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = (\n",
    "    \"https://gist.githubusercontent.com/rsnirwan/a8b424085c9f44ef2598da74ce43e7a3\"\n",
    "    \"/raw/b6fdef21fe1f654787fa0493846c546b7f9c4df2/ts_long.csv\"\n",
    ")\n",
    "df = pd.read_csv(url, index_col=0, parse_dates=True)\n",
    "df.head()\n",
    "for col in df_long.columns:\n",
    "    # Check if column is not of string type\n",
    "    if df_long[col].dtype != 'object' and pd.api.types.is_string_dtype(df_long[col]) == False:\n",
    "        df_long[col] = df_long[col].astype('float32')\n",
    "dataset = PandasDataset.from_long_dataframe(df, target=\"target\", item_id=\"item_id\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecasts, tss = get_lag_llama_predictions(dataset, 12, torch.device(\"cpu\"))\n",
    "print(forecasts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
